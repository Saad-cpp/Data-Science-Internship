{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNR+vsb2IIYaTGLEOj8lcGu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saad-cpp/Data-Science-Internship/blob/main/Basic_GraphRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9-me5gkMvLx",
        "outputId": "ac122beb-7ca5-4a90-c692-bb79f4804e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-vectors-unstructured'...\n",
            "remote: Enumerating objects: 178, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 178 (delta 29), reused 80 (delta 17), pack-reused 69 (from 1)\u001b[K\n",
            "Receiving objects: 100% (178/178), 47.99 MiB | 18.14 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://token@github.com/neo4j-graphacademy/llm-vectors-unstructured"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/llm-vectors-unstructured/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yKhcykluNK1b",
        "outputId": "63201a9e-9138-4635-9a8d-9a761404cf4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv (from -r /content/llm-vectors-unstructured/requirements.txt (line 1))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting openai (from -r /content/llm-vectors-unstructured/requirements.txt (line 2))\n",
            "  Downloading openai-1.51.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting langchain (from -r /content/llm-vectors-unstructured/requirements.txt (line 3))\n",
            "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_openai (from -r /content/llm-vectors-unstructured/requirements.txt (line 4))\n",
            "  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_community (from -r /content/llm-vectors-unstructured/requirements.txt (line 5))\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting neo4j (from -r /content/llm-vectors-unstructured/requirements.txt (line 6))\n",
            "  Downloading neo4j-5.25.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from -r /content/llm-vectors-unstructured/requirements.txt (line 7)) (0.17.1)\n",
            "Collecting colorama (from -r /content/llm-vectors-unstructured/requirements.txt (line 8))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2))\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2))\n",
            "  Downloading jiter-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (3.10.8)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.8 (from langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3))\n",
            "  Downloading langchain_core-0.3.9-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3))\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3))\n",
            "  Downloading langsmith-0.1.132-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3))\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai->-r /content/llm-vectors-unstructured/requirements.txt (line 4))\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community->-r /content/llm-vectors-unstructured/requirements.txt (line 5))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community->-r /content/llm-vectors-unstructured/requirements.txt (line 5))\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j->-r /content/llm-vectors-unstructured/requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob->-r /content/llm-vectors-unstructured/requirements.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r /content/llm-vectors-unstructured/requirements.txt (line 5))\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r /content/llm-vectors-unstructured/requirements.txt (line 5))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2))\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.8->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.8->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3))\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3))\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->-r /content/llm-vectors-unstructured/requirements.txt (line 7)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->-r /content/llm-vectors-unstructured/requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->-r /content/llm-vectors-unstructured/requirements.txt (line 7)) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r /content/llm-vectors-unstructured/requirements.txt (line 2)) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3)) (3.1.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->langchain->-r /content/llm-vectors-unstructured/requirements.txt (line 3))\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community->-r /content/llm-vectors-unstructured/requirements.txt (line 5))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading openai-1.51.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading neo4j-5.25.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.9-py3-none-any.whl (401 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.132-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: tenacity, python-dotenv, orjson, neo4j, mypy-extensions, marshmallow, jsonpointer, jiter, h11, colorama, typing-inspect, tiktoken, requests-toolbelt, jsonpatch, httpcore, pydantic-settings, httpx, dataclasses-json, openai, langsmith, langchain-core, langchain-text-splitters, langchain_openai, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed colorama-0.4.6 dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.2 langchain-core-0.3.9 langchain-text-splitters-0.3.0 langchain_community-0.3.1 langchain_openai-0.2.2 langsmith-0.1.132 marshmallow-3.22.0 mypy-extensions-1.0.0 neo4j-5.25.0 openai-1.51.1 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5vuFG3z7NZrL",
        "outputId": "9214df39-ebc6-4e99-bb90-56c579e01c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.23.4)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "gem_key = userdata.get('GEMINI-KEY')\n",
        "neo_url = userdata.get('neo-url')\n",
        "neo_user = userdata.get('neo-user')\n",
        "neo_pass = userdata.get('neo-pass')"
      ],
      "metadata": {
        "id": "ewsFP9CcNexr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import google.generativeai as genai\n",
        "from neo4j import GraphDatabase\n",
        "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
        "\n",
        "genai.configure(api_key=gem_key)\n",
        "\n",
        "COURSES_PATH = \"/content/llm-vectors-unstructured/llm-vectors-unstructured/data/asciidoc\"\n",
        "\n",
        "loader = DirectoryLoader(COURSES_PATH, glob=\"**/lesson.adoc\", loader_cls=TextLoader)\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=1500,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9GzDiDpNE-S",
        "outputId": "df7b0700-bbfc-4330-87a3-9d80fce0ea0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2848, which is longer than the specified 1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "V7kg-IjFPjBk",
        "outputId": "c94e35e8-17c9-44ae-fe87-26ff4c1924fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting google-generativeai<0.9.0,>=0.8.0 (from langchain-google-genai)\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (2.9.2)\n",
            "Collecting google-ai-generativelanguage==0.6.10 (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.10-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.24.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai) (0.1.132)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.23.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai) (1.2.2)\n",
            "Downloading langchain_google_genai-2.0.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.6.10-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.0/760.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-ai-generativelanguage, google-generativeai, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.6\n",
            "    Uninstalling google-ai-generativelanguage-0.6.6:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.6\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.7.2\n",
            "    Uninstalling google-generativeai-0.7.2:\n",
            "      Successfully uninstalled google-generativeai-0.7.2\n",
            "Successfully installed google-ai-generativelanguage-0.6.10 google-generativeai-0.8.3 langchain-google-genai-2.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "f21814a25ab44237ac1976bf5c6c2a9a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREATING VECTOR FROM DOCUMENTS**"
      ],
      "metadata": {
        "id": "UO22CNLAzuSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "neo4j_db = Neo4jVector.from_documents(\n",
        "    chunks,\n",
        "    GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key = gem_key),\n",
        "    url=neo_url,\n",
        "    username=neo_user,\n",
        "    password=neo_pass,\n",
        "    database=\"neo4j\",\n",
        "    index_name=\"chunkVector\",\n",
        "    node_label=\"Chunk\",\n",
        "    text_node_property=\"text\",\n",
        "    embedding_node_property=\"embedding\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "RT2elPEWPKz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = genai.embed_content(model=\"models/text-embedding-004\", content=\"What does hallucination mean?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "OIOnPQnsQ0Gm",
        "outputId": "14d15bbb-6a89-411c-f37c-94b3b45791ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0119056925, 0.0048198216, -0.03259242, -0.044548422, -0.07073696, 0.022851354, 0.006630087, -0.02595581, -0.08095177, -0.015849285, 0.033063997, -0.0042900923, 0.0028481234, -0.02006189, -0.04180651, -0.077049285, 0.023721687, 0.062067565, -0.03783016, -0.009402408, -0.015281742, -0.028301034, 0.0073960163, -0.09713471, -0.0224557, 0.0009633503, 0.029324098, 0.026870906, -0.036750205, 0.042385515, 0.05897531, -0.010236744, 0.072089665, 0.04424017, 0.04958447, 0.044356085, 0.011106528, 0.04393049, -0.000107527114, -0.064931884, -0.038579054, 0.013928379, -0.05128461, 0.037999574, -0.071024135, 0.08157883, 0.015119018, 0.06429069, 0.009956238, -0.0076128882, -0.009618708, 0.027581543, -0.039176147, 0.07421379, -0.03317814, -0.0030371428, -0.026351376, -0.027670817, 0.001293356, 0.0014247712, -0.05697347, 0.015356495, -0.027492633, 1.1410143e-05, -0.04104694, -0.013220434, -0.052097112, -0.036123518, -0.049761225, 0.038424842, -0.041618172, 0.051542133, -0.057390854, 0.0073156497, -0.035016272, -0.017534716, 0.023238458, -0.022595009, -0.04346939, 0.05032755, -0.015832227, 0.029673392, 0.06799369, 0.05891729, 0.046323463, 0.016140403, 0.0022505447, -0.03619653, -0.07044347, 0.019845285, 0.07365347, 0.014400337, -0.07837102, -0.034367662, -0.025434084, -0.009363208, 0.012064713, -0.06466444, 0.0003980141, 0.011912053, -0.019377183, 0.0091110775, -0.02824957, 0.0016270856, 0.03554949, 0.049569923, 0.011126691, -0.017804328, -0.033933096, -0.032531362, 0.0075634355, -0.058650076, 0.066161975, 0.044126052, -0.018936358, 4.7362057e-05, -0.048826065, 0.010518585, -0.0440354, -0.06555758, 0.016110115, 0.024107456, -0.018810641, -0.02002853, 0.023834888, 0.041838925, 0.020321619, 0.00039722474, -0.06003267, -0.013675146, 0.029190956, -0.0942741, 0.0074014557, 0.005293088, -0.009551262, 0.015399319, 0.036236763, 0.015618829, 0.018215066, 0.033814788, 0.074666865, -0.04786272, -0.017734753, -0.008828313, 0.01340665, -0.07202592, -0.053761635, 0.008750878, 0.057049915, 0.037341155, -0.028688682, 0.020453509, 0.036140695, -0.012373815, 0.041175675, 0.04744881, 0.022186698, 0.042574514, 0.048168834, 0.0042271265, -0.043871563, -0.009614418, -0.021171395, -0.012915315, -0.0316984, 0.053810567, -0.022428494, -0.034021482, 0.060898636, 0.000749426, 0.00064874295, -0.028955525, 0.009300957, -0.06228782, 0.06404614, -0.008999951, -0.006710268, 0.020682955, -0.082640916, 0.028055368, 0.050041962, 0.019393966, 0.07984085, -0.046652187, 0.0134519525, 0.112749904, -0.007392111, -0.06520959, 0.056241803, 0.007910133, -0.0015107591, -0.0018465116, -0.022501256, 0.016311843, -0.011611867, -0.011689372, -0.014284841, -0.021123257, 0.012857797, -0.006811733, 0.080416754, 0.03368352, 0.06360955, 0.02591819, -0.009117707, -0.02000942, -0.041534342, -0.015810883, 0.07156354, 0.0066419947, 0.0044471594, -0.014661538, 0.01741478, -0.050253052, -0.0076317214, -0.020431405, 0.07776156, 0.009535103, 0.059531666, -0.063044675, -0.028125223, 0.023151614, 0.0003346773, 0.035185017, -0.03671319, -0.008438143, -0.08458341, -0.033291597, 0.03742469, 0.035056937, -0.044706192, -0.014844675, -0.024412861, -0.0067807976, -0.032607023, -0.020438405, 0.0006566597, 0.009362115, 0.003299508, -0.019919258, 0.0100248465, -0.01364861, 0.013368414, 0.017122911, 0.032329556, -0.013597854, 0.038649537, 0.030332027, -0.07130078, -0.058565732, 0.004647593, -0.0804734, -0.024315333, 0.031041406, -0.034303926, 0.014886061, 0.048254672, -0.046021905, -0.058152847, -0.0185617, 0.050918743, -0.028745918, -0.043680474, -0.046703562, -0.040130995, -0.08408421, -0.006639804, 0.013046318, 0.010480273, -0.0011139562, 0.017075192, 0.018390294, -0.09054783, -0.03001621, -0.007827864, 0.062115528, 0.021210382, 0.0038139885, 0.00469584, -0.03329493, -0.034989215, -0.030491773, 1.2444006e-05, 0.005381004, 0.013275237, -0.02314619, -0.05145807, 0.025677571, -0.0090409275, -0.033265933, -0.00069613144, 0.06647536, 0.050271854, -0.007490493, 0.020400587, -0.011932555, 0.0519302, 0.03764421, -0.023469219, 0.021099303, -0.017598748, 0.084322885, -0.023388224, -0.028315408, 0.0024424233, 0.020799875, -0.005948072, 0.00021523207, -0.01059961, -0.026046067, 0.042341813, -1.5384012e-05, -0.10909148, -0.054317016, -0.054272737, -0.0043618847, -0.04293497, -0.046550643, 0.0458669, -0.05688353, 0.011066851, 0.040243234, 0.010711051, 0.019067993, 0.01605121, -0.024275094, -0.02286687, -0.033278894, -0.0062706065, 0.0063742804, 0.05210945, 0.05881989, 0.005919273, -0.030565728, 0.04128812, -0.008909088, -0.10417183, 0.026383257, 0.027796526, 0.036772143, -0.031345695, 0.007102893, 0.05422209, 0.009451058, -0.013546059, -0.05119952, -0.021821834, -0.06903918, -0.044255197, -0.060818065, 0.033416104, 0.043122884, -0.030657573, -0.015970247, 0.03594522, -0.052767288, 0.06956527, -0.01398538, -0.015795901, -0.020160107, -0.026821682, 0.01160946, 0.05323758, -0.0057106544, 0.0032123835, 0.02241571, -0.025025921, 0.045801714, 0.0015197723, -0.039795764, -0.0047843824, -0.047614995, -0.044240665, -0.038123854, 0.02458103, -0.008041571, 0.0013791033, -0.051765304, 0.0057179653, -0.04590475, -0.051840816, -0.0655996, -0.002537177, 0.039119113, -0.017130058, 0.020839816, -0.045510426, -0.025543448, 0.035143245, 0.014111174, -0.023916198, 0.0032424245, 0.006877735, -0.01832966, 0.016014345, 0.033973224, -0.043294974, -0.04860533, 0.050693147, -0.052569136, 0.009521512, -0.037883874, 0.15200855, -0.0034328683, 0.026742322, 0.026165042, 0.030099418, 0.014381637, -0.010696131, -0.036224034, 0.013963268, -0.039645594, -0.028080843, -0.012135416, -0.06183928, -0.005108199, -0.05763802, -0.011603934, 0.021577813, 0.008220188, -0.004120403, -0.022605207, 0.025573712, -0.030578487, -0.0006339014, -0.055228796, 0.01699339, 0.02695897, -0.051756654, 0.04795282, -0.0018121013, 0.023729553, -0.036182668, 0.008681873, 0.013679121, 0.004555077, 0.017804282, 0.024862044, -0.036315322, -0.0046548136, -0.038430344, -0.056217685, 0.038191, 0.031489503, -0.022423156, 0.01792702, 0.04965368, -0.0080588525, -0.022353662, -0.044883393, 0.025277326, 0.037884016, 0.009654782, -0.03706061, -0.06442828, 0.032586742, -0.0073054777, -0.0066272025, 0.0151773505, 0.029163277, -0.019200366, -0.019813787, 0.10861781, 0.008551032, 0.038421713, 0.0026084082, 0.004127186, -0.016338924, -0.0073606437, 0.017824221, -0.0076369676, -0.03545786, 0.07717164, 0.008413999, 0.0064855306, -0.022644153, -0.029068805, 0.019110166, -0.008974275, -0.018842602, -0.018629808, 0.01905706, -0.06632252, 0.0027145536, 0.039618246, -0.016096381, -0.044370696, 0.09192218, 0.016590869, -0.016335322, -0.030908197, 0.008764138, -0.05107602, -0.046953127, -0.030757876, -0.007324305, -0.035288006, -0.058260523, 0.045525905, -0.017271273, 0.02458095, 0.04607341, -0.08939266, -0.02135717, 0.012825329, -0.019639418, -0.008614658, -0.0392685, 0.031885706, 0.0090696495, 0.0071692984, 0.004938794, -0.008225923, 0.055210315, 0.0056984285, 0.023290887, -0.011853231, 0.052326262, -0.010952372, 0.05464174, -0.019920027, -0.015836647, 0.0071502035, -0.03940109, 0.017697193, 0.013640451, -0.020188332, 0.026774546, 0.054224797, 0.016883345, -0.020105792, 0.04280891, 0.004636894, -0.043693267, 0.008780447, 0.08180971, -0.011518918, 0.04597968, -0.007119385, 0.036515377, -0.00054972817, -0.018081585, 0.036731355, 0.0035955242, 0.008144295, -0.011214818, -0.001166021, 0.036584597, -0.011746198, 0.006970792, 0.03202298, -0.039603766, -0.008859432, -0.004229421, -0.005642432, 0.013838302, 0.020839691, 0.010909582, 0.039155573, -0.029433602, 0.030886384, 0.0011352716, 0.04912273, 0.0031741909, 0.0103046475, 0.003678223, -0.049252503, -0.0020023969, -0.031146513, -0.026060164, 0.0013223144, 0.040867962, -0.07568897, -0.010791296, 0.012819977, 0.017525604, -0.017891122, -0.028069085, 0.011684938, 0.01072854, 0.002779126, 0.015429351, 0.013257102, 0.006843072, -0.00026942068, -0.011287767, -0.025440156, -0.0118503515, 0.024486823, 0.01494749, -0.027270045, 0.067648984, 0.0031866361, -0.04169001, 0.035186928, -0.0035106752, -0.020148538, 0.032970242, 0.04310075, 0.07170296, 0.01628979, -0.027142894, 0.039012834, -0.06157502, -0.0070613576, -0.001931391, -0.020499174, -0.02753094, 0.027184427, -0.031929553, -0.04523742, 0.03282567, -0.028564064, -0.013739466, -0.011473384, 0.04577357, -0.058695897, 0.0073007173, 0.00930657, 0.017430479, 0.035014108, 0.06647263, -0.023442136, -0.025028273, -0.06982384, -0.027350932, -0.014206481, -0.018833611, 0.020181464, 0.015325731, -0.03151106, 0.002045748, 0.030809443, 0.03408746, -0.011803787, 0.0132701425, -0.010232242, -0.021908712, -0.002957373, -0.014967224, -0.023473864, 0.020466639, 0.018377747, 0.01876068, -0.04844185, -0.009171715, -0.040751927, -0.056063857, -0.0059017893, -0.044980306, -0.029671123, 0.010962093, -0.038716756, 0.021832507, -0.02563093, -0.03497777, 0.007863253, -0.06789572, 0.0110049145, -0.033282794, 0.011962657, 0.009615952, 0.02861644, 0.04055809, 0.029317737, -0.012394975, 0.0051038875, 0.063136496, 0.0010110381, 0.0010620289, -0.051387127, 0.045365535, -0.02485178, -0.06044159, -0.013277818, 0.035781316, 0.016579477, 0.01105046, -0.03618921, -0.023629999, 0.0088188145, -0.018993016, -0.014598083, -0.015266866, 0.034614746, -0.020324934, 0.016084952, 0.018791901, 0.009989903, 0.018242374, 0.03538953, -0.010601651, -0.023133228, 0.019234713, 0.0114470925, 0.06673922, -0.036308046, 0.023401124, -0.004411492, -0.03471435, -0.04241031, -0.0012515987, -0.04522594, -0.012749763, 0.028531438, 0.026889013, 0.04100081, -0.0005104964, -0.0253647, 0.0735128, -0.004623553, 0.019063618, 0.015878813, -0.03161162, 0.006879205, 0.06022509, 0.049471352, -0.02312329, 0.0005726241, -0.035033662, 0.07363373, -0.07781331, -0.01544635, -0.03317014, -0.004645804, 0.009165552, -0.02258899, 0.006344956, -0.030988568, 0.047751635, 0.040029235, 0.045492038, -0.038927376, 0.019290254, -0.013675378, -0.0017190785, 0.07276276, 0.041662354, -0.0154314535, 0.07486979, -0.024116028, -0.04704968, 0.035161473, 0.009726136, 0.031749945, -0.003395842, -0.048552662, -0.064698435, -0.004275628, 0.0014218752, -0.058706004, -8.141102e-05, 0.024261672, 0.017058376, 0.02510874, 0.032452542, 0.03165527, -0.0031921018, -0.02725205, -0.008836042, 0.012357539, 0.092094555, -0.026149008, 0.029516598, -0.018353863, 0.021798054, 0.038232844, -0.015025468]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['embedding'])\n",
        "embedding = response['embedding']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc7cg4uWS8A2",
        "outputId": "f3478c9d-cbcc-4fff-eda3-7e24dca0750a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0119056925, 0.0048198216, -0.03259242, -0.044548422, -0.07073696, 0.022851354, 0.006630087, -0.02595581, -0.08095177, -0.015849285, 0.033063997, -0.0042900923, 0.0028481234, -0.02006189, -0.04180651, -0.077049285, 0.023721687, 0.062067565, -0.03783016, -0.009402408, -0.015281742, -0.028301034, 0.0073960163, -0.09713471, -0.0224557, 0.0009633503, 0.029324098, 0.026870906, -0.036750205, 0.042385515, 0.05897531, -0.010236744, 0.072089665, 0.04424017, 0.04958447, 0.044356085, 0.011106528, 0.04393049, -0.000107527114, -0.064931884, -0.038579054, 0.013928379, -0.05128461, 0.037999574, -0.071024135, 0.08157883, 0.015119018, 0.06429069, 0.009956238, -0.0076128882, -0.009618708, 0.027581543, -0.039176147, 0.07421379, -0.03317814, -0.0030371428, -0.026351376, -0.027670817, 0.001293356, 0.0014247712, -0.05697347, 0.015356495, -0.027492633, 1.1410143e-05, -0.04104694, -0.013220434, -0.052097112, -0.036123518, -0.049761225, 0.038424842, -0.041618172, 0.051542133, -0.057390854, 0.0073156497, -0.035016272, -0.017534716, 0.023238458, -0.022595009, -0.04346939, 0.05032755, -0.015832227, 0.029673392, 0.06799369, 0.05891729, 0.046323463, 0.016140403, 0.0022505447, -0.03619653, -0.07044347, 0.019845285, 0.07365347, 0.014400337, -0.07837102, -0.034367662, -0.025434084, -0.009363208, 0.012064713, -0.06466444, 0.0003980141, 0.011912053, -0.019377183, 0.0091110775, -0.02824957, 0.0016270856, 0.03554949, 0.049569923, 0.011126691, -0.017804328, -0.033933096, -0.032531362, 0.0075634355, -0.058650076, 0.066161975, 0.044126052, -0.018936358, 4.7362057e-05, -0.048826065, 0.010518585, -0.0440354, -0.06555758, 0.016110115, 0.024107456, -0.018810641, -0.02002853, 0.023834888, 0.041838925, 0.020321619, 0.00039722474, -0.06003267, -0.013675146, 0.029190956, -0.0942741, 0.0074014557, 0.005293088, -0.009551262, 0.015399319, 0.036236763, 0.015618829, 0.018215066, 0.033814788, 0.074666865, -0.04786272, -0.017734753, -0.008828313, 0.01340665, -0.07202592, -0.053761635, 0.008750878, 0.057049915, 0.037341155, -0.028688682, 0.020453509, 0.036140695, -0.012373815, 0.041175675, 0.04744881, 0.022186698, 0.042574514, 0.048168834, 0.0042271265, -0.043871563, -0.009614418, -0.021171395, -0.012915315, -0.0316984, 0.053810567, -0.022428494, -0.034021482, 0.060898636, 0.000749426, 0.00064874295, -0.028955525, 0.009300957, -0.06228782, 0.06404614, -0.008999951, -0.006710268, 0.020682955, -0.082640916, 0.028055368, 0.050041962, 0.019393966, 0.07984085, -0.046652187, 0.0134519525, 0.112749904, -0.007392111, -0.06520959, 0.056241803, 0.007910133, -0.0015107591, -0.0018465116, -0.022501256, 0.016311843, -0.011611867, -0.011689372, -0.014284841, -0.021123257, 0.012857797, -0.006811733, 0.080416754, 0.03368352, 0.06360955, 0.02591819, -0.009117707, -0.02000942, -0.041534342, -0.015810883, 0.07156354, 0.0066419947, 0.0044471594, -0.014661538, 0.01741478, -0.050253052, -0.0076317214, -0.020431405, 0.07776156, 0.009535103, 0.059531666, -0.063044675, -0.028125223, 0.023151614, 0.0003346773, 0.035185017, -0.03671319, -0.008438143, -0.08458341, -0.033291597, 0.03742469, 0.035056937, -0.044706192, -0.014844675, -0.024412861, -0.0067807976, -0.032607023, -0.020438405, 0.0006566597, 0.009362115, 0.003299508, -0.019919258, 0.0100248465, -0.01364861, 0.013368414, 0.017122911, 0.032329556, -0.013597854, 0.038649537, 0.030332027, -0.07130078, -0.058565732, 0.004647593, -0.0804734, -0.024315333, 0.031041406, -0.034303926, 0.014886061, 0.048254672, -0.046021905, -0.058152847, -0.0185617, 0.050918743, -0.028745918, -0.043680474, -0.046703562, -0.040130995, -0.08408421, -0.006639804, 0.013046318, 0.010480273, -0.0011139562, 0.017075192, 0.018390294, -0.09054783, -0.03001621, -0.007827864, 0.062115528, 0.021210382, 0.0038139885, 0.00469584, -0.03329493, -0.034989215, -0.030491773, 1.2444006e-05, 0.005381004, 0.013275237, -0.02314619, -0.05145807, 0.025677571, -0.0090409275, -0.033265933, -0.00069613144, 0.06647536, 0.050271854, -0.007490493, 0.020400587, -0.011932555, 0.0519302, 0.03764421, -0.023469219, 0.021099303, -0.017598748, 0.084322885, -0.023388224, -0.028315408, 0.0024424233, 0.020799875, -0.005948072, 0.00021523207, -0.01059961, -0.026046067, 0.042341813, -1.5384012e-05, -0.10909148, -0.054317016, -0.054272737, -0.0043618847, -0.04293497, -0.046550643, 0.0458669, -0.05688353, 0.011066851, 0.040243234, 0.010711051, 0.019067993, 0.01605121, -0.024275094, -0.02286687, -0.033278894, -0.0062706065, 0.0063742804, 0.05210945, 0.05881989, 0.005919273, -0.030565728, 0.04128812, -0.008909088, -0.10417183, 0.026383257, 0.027796526, 0.036772143, -0.031345695, 0.007102893, 0.05422209, 0.009451058, -0.013546059, -0.05119952, -0.021821834, -0.06903918, -0.044255197, -0.060818065, 0.033416104, 0.043122884, -0.030657573, -0.015970247, 0.03594522, -0.052767288, 0.06956527, -0.01398538, -0.015795901, -0.020160107, -0.026821682, 0.01160946, 0.05323758, -0.0057106544, 0.0032123835, 0.02241571, -0.025025921, 0.045801714, 0.0015197723, -0.039795764, -0.0047843824, -0.047614995, -0.044240665, -0.038123854, 0.02458103, -0.008041571, 0.0013791033, -0.051765304, 0.0057179653, -0.04590475, -0.051840816, -0.0655996, -0.002537177, 0.039119113, -0.017130058, 0.020839816, -0.045510426, -0.025543448, 0.035143245, 0.014111174, -0.023916198, 0.0032424245, 0.006877735, -0.01832966, 0.016014345, 0.033973224, -0.043294974, -0.04860533, 0.050693147, -0.052569136, 0.009521512, -0.037883874, 0.15200855, -0.0034328683, 0.026742322, 0.026165042, 0.030099418, 0.014381637, -0.010696131, -0.036224034, 0.013963268, -0.039645594, -0.028080843, -0.012135416, -0.06183928, -0.005108199, -0.05763802, -0.011603934, 0.021577813, 0.008220188, -0.004120403, -0.022605207, 0.025573712, -0.030578487, -0.0006339014, -0.055228796, 0.01699339, 0.02695897, -0.051756654, 0.04795282, -0.0018121013, 0.023729553, -0.036182668, 0.008681873, 0.013679121, 0.004555077, 0.017804282, 0.024862044, -0.036315322, -0.0046548136, -0.038430344, -0.056217685, 0.038191, 0.031489503, -0.022423156, 0.01792702, 0.04965368, -0.0080588525, -0.022353662, -0.044883393, 0.025277326, 0.037884016, 0.009654782, -0.03706061, -0.06442828, 0.032586742, -0.0073054777, -0.0066272025, 0.0151773505, 0.029163277, -0.019200366, -0.019813787, 0.10861781, 0.008551032, 0.038421713, 0.0026084082, 0.004127186, -0.016338924, -0.0073606437, 0.017824221, -0.0076369676, -0.03545786, 0.07717164, 0.008413999, 0.0064855306, -0.022644153, -0.029068805, 0.019110166, -0.008974275, -0.018842602, -0.018629808, 0.01905706, -0.06632252, 0.0027145536, 0.039618246, -0.016096381, -0.044370696, 0.09192218, 0.016590869, -0.016335322, -0.030908197, 0.008764138, -0.05107602, -0.046953127, -0.030757876, -0.007324305, -0.035288006, -0.058260523, 0.045525905, -0.017271273, 0.02458095, 0.04607341, -0.08939266, -0.02135717, 0.012825329, -0.019639418, -0.008614658, -0.0392685, 0.031885706, 0.0090696495, 0.0071692984, 0.004938794, -0.008225923, 0.055210315, 0.0056984285, 0.023290887, -0.011853231, 0.052326262, -0.010952372, 0.05464174, -0.019920027, -0.015836647, 0.0071502035, -0.03940109, 0.017697193, 0.013640451, -0.020188332, 0.026774546, 0.054224797, 0.016883345, -0.020105792, 0.04280891, 0.004636894, -0.043693267, 0.008780447, 0.08180971, -0.011518918, 0.04597968, -0.007119385, 0.036515377, -0.00054972817, -0.018081585, 0.036731355, 0.0035955242, 0.008144295, -0.011214818, -0.001166021, 0.036584597, -0.011746198, 0.006970792, 0.03202298, -0.039603766, -0.008859432, -0.004229421, -0.005642432, 0.013838302, 0.020839691, 0.010909582, 0.039155573, -0.029433602, 0.030886384, 0.0011352716, 0.04912273, 0.0031741909, 0.0103046475, 0.003678223, -0.049252503, -0.0020023969, -0.031146513, -0.026060164, 0.0013223144, 0.040867962, -0.07568897, -0.010791296, 0.012819977, 0.017525604, -0.017891122, -0.028069085, 0.011684938, 0.01072854, 0.002779126, 0.015429351, 0.013257102, 0.006843072, -0.00026942068, -0.011287767, -0.025440156, -0.0118503515, 0.024486823, 0.01494749, -0.027270045, 0.067648984, 0.0031866361, -0.04169001, 0.035186928, -0.0035106752, -0.020148538, 0.032970242, 0.04310075, 0.07170296, 0.01628979, -0.027142894, 0.039012834, -0.06157502, -0.0070613576, -0.001931391, -0.020499174, -0.02753094, 0.027184427, -0.031929553, -0.04523742, 0.03282567, -0.028564064, -0.013739466, -0.011473384, 0.04577357, -0.058695897, 0.0073007173, 0.00930657, 0.017430479, 0.035014108, 0.06647263, -0.023442136, -0.025028273, -0.06982384, -0.027350932, -0.014206481, -0.018833611, 0.020181464, 0.015325731, -0.03151106, 0.002045748, 0.030809443, 0.03408746, -0.011803787, 0.0132701425, -0.010232242, -0.021908712, -0.002957373, -0.014967224, -0.023473864, 0.020466639, 0.018377747, 0.01876068, -0.04844185, -0.009171715, -0.040751927, -0.056063857, -0.0059017893, -0.044980306, -0.029671123, 0.010962093, -0.038716756, 0.021832507, -0.02563093, -0.03497777, 0.007863253, -0.06789572, 0.0110049145, -0.033282794, 0.011962657, 0.009615952, 0.02861644, 0.04055809, 0.029317737, -0.012394975, 0.0051038875, 0.063136496, 0.0010110381, 0.0010620289, -0.051387127, 0.045365535, -0.02485178, -0.06044159, -0.013277818, 0.035781316, 0.016579477, 0.01105046, -0.03618921, -0.023629999, 0.0088188145, -0.018993016, -0.014598083, -0.015266866, 0.034614746, -0.020324934, 0.016084952, 0.018791901, 0.009989903, 0.018242374, 0.03538953, -0.010601651, -0.023133228, 0.019234713, 0.0114470925, 0.06673922, -0.036308046, 0.023401124, -0.004411492, -0.03471435, -0.04241031, -0.0012515987, -0.04522594, -0.012749763, 0.028531438, 0.026889013, 0.04100081, -0.0005104964, -0.0253647, 0.0735128, -0.004623553, 0.019063618, 0.015878813, -0.03161162, 0.006879205, 0.06022509, 0.049471352, -0.02312329, 0.0005726241, -0.035033662, 0.07363373, -0.07781331, -0.01544635, -0.03317014, -0.004645804, 0.009165552, -0.02258899, 0.006344956, -0.030988568, 0.047751635, 0.040029235, 0.045492038, -0.038927376, 0.019290254, -0.013675378, -0.0017190785, 0.07276276, 0.041662354, -0.0154314535, 0.07486979, -0.024116028, -0.04704968, 0.035161473, 0.009726136, 0.031749945, -0.003395842, -0.048552662, -0.064698435, -0.004275628, 0.0014218752, -0.058706004, -8.141102e-05, 0.024261672, 0.017058376, 0.02510874, 0.032452542, 0.03165527, -0.0031921018, -0.02725205, -0.008836042, 0.012357539, 0.092094555, -0.026149008, 0.029516598, -0.018353863, 0.021798054, 0.038232844, -0.015025468]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.graphs import Neo4jGraph\n",
        "\n",
        "graph = Neo4jGraph(\n",
        "    url=neo_url,\n",
        "    username=neo_user,\n",
        "    password=neo_pass,\n",
        ")"
      ],
      "metadata": {
        "id": "y4-RVsVkSUkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = graph.query(\"\"\"\n",
        "CALL db.index.vector.queryNodes('chunkVector', 6, $embedding)\n",
        "YIELD node, score\n",
        "RETURN node.text, score\n",
        "\"\"\", {\"embedding\": embedding})\n"
      ],
      "metadata": {
        "id": "4X82Gl3nSnK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in result:\n",
        "    print(row['node.text'], row['score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PA9GfYcQTRpH",
        "outputId": "f7a0232e-1523-4a1f-ffe7-e67706feffcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= Avoiding Hallucination\n",
            ":order: 2\n",
            ":type: lesson\n",
            "\n",
            "As you learned in the previous lesson, LLMs can \"make things up\".\n",
            "\n",
            "LLMs are designed to generate human-like text based on the patterns they've identified in vast amounts of data. \n",
            "\n",
            "Due to their reliance on patterns and the sheer volume of training information, LLMs sometimes **hallucinate** or produce outputs that manifest as generating untrue facts, asserting details with unwarranted confidence, or crafting plausible yet nonsensical explanations.\n",
            "\n",
            "These manifestations arise from a mix of _overfitting_, biases in the training data, and the model's attempt to generalize from vast amounts of information.\n",
            "\n",
            "== Common Hallucination Problems\n",
            "\n",
            "Let's take a closer look at some reasons why this may occur.\n",
            "\n",
            "=== Temperature\n",
            "\n",
            "LLMs have a _temperature_, corresponding to the amount of randomness the underlying model should use when generating the text.\n",
            "\n",
            "The higher the temperature value, the more random the generated result will become, and the more likely the response will contain false statements.\n",
            "\n",
            "A higher temperature may be appropriate when configuring an LLM to respond with more diverse and creative outputs, but it comes at the expense of consistency and precision.\n",
            "\n",
            "For example, a higher temperature may be suitable for constructing a work of fiction or a novel joke.\n",
            "\n",
            "On the other hand, a lower temperature, even `0`, is required when a response grounded in facts is essential. 0.7975003719329834\n",
            "LLMs are fine-tuned to be as helpful as possible, even if that means occasionally generating misleading or baseless content, a phenomenon known as **hallucination**.\n",
            "\n",
            "For example, when asked to _\"Describe the moon.\"_ an LLM may respond with _\"The moon is made of cheese.\"_. While this is a common saying, it is not true.\n",
            "\n",
            "image::images/confused-llm.svg[A diagram of a confused LLM with a question mark thinking about the moon and cheese.]\n",
            "\n",
            "While LLMs can represent the essence of words and phrases, they don't possess a genuine understanding or ethical judgment of the content.\n",
            "\n",
            "Large Language Models (LLMs) are often considered \"black boxes\" due to the difficulty deciphering their decision-making processes.\n",
            "The LLM would also be unable to provide the sources for its output or explain its reasoning.\n",
            "\n",
            "image::images/llm-blackbox.svg[An LLM as a black box, responding to the question 'How did you determine that answer?' with 'I don't know.']\n",
            "\n",
            "These factors can lead to outputs that might be biased, devoid of context, or lack logical coherence.\n",
            "\n",
            "=== Fixing Hallucinations\n",
            "\n",
            "Providing additional *contextual* data helps to _ground_ the LLM's responses and make them more accurate.\n",
            "\n",
            "A knowledge graph is a mechanism for providing additional data to an LLM. \n",
            "Data within the knowledge graph can guide the LLM to provide more relevant, accurate, and reliable responses. 0.779241681098938\n",
            "A knowledge graph is a mechanism for providing additional data to an LLM. \n",
            "Data within the knowledge graph can guide the LLM to provide more relevant, accurate, and reliable responses. \n",
            "\n",
            "While the LLM uses its language skills to interpret and respond to the contextual data, it will not disregard the original training data.\n",
            "\n",
            "You can think of the original training data as the base knowledge and linguistic capabilities, while the contextual information guides in specific situations.\n",
            "\n",
            "The combination of both approaches enables the LLM to generate more meaningful responses.\n",
            "\n",
            "== Check Your Understanding\n",
            "\n",
            "include::questions/1-hallucination.adoc[leveloffset=+1]\n",
            "include::questions/2-fixing-hallucination.adoc[leveloffset=+1]\n",
            "\n",
            "\n",
            "[.summary]\n",
            "== Lesson Summary\n",
            "\n",
            "In this lesson, you learned about LLMs, their benefits, and challenges.\n",
            "\n",
            "In the next lesson, you will learn about hallucination and the strategies for avoiding it. 0.7589590549468994\n",
            "= Grounding LLMs\n",
            ":order: 3\n",
            ":type: lesson\n",
            "\n",
            "In the previous lesson, you learned about the potential methods that you could use to avoid _Hallucination_, including _Grounding_.\n",
            "\n",
            "In this lesson, you will explore grounding and an approach known as _Retrieval Augmented Generation_ (RAG).\n",
            "\n",
            "[.video]\n",
            "video::Le_j18w2oGI[youtube,width=560,height=315]\n",
            "\n",
            "[.transcript]\n",
            "== Grounding LLMs with RAG\n",
            "\n",
            "Grounding is the process of providing context to an LLM to improve the accuracy of its responses and reduce the likelihood of hallucinations. \n",
            "For developers and data scientists, grounding usually offers the highest impact for the lowest effort.\n",
            "\n",
            "Training a Large Language Model has a high computational cost. \n",
            "According to Wikipedia, OpenAI's GPT-3 model was link:https://en.wikipedia.org/wiki/GPT-3[trained on 175 billion parameters^], and the resulting trained model takes 800GB to store.\n",
            "\n",
            "Retraining a model on new data would be expensive and time-consuming. \n",
            "A model may take weeks or months to train.\n",
            "\n",
            "For example, if you were building a chatbot for a news agency it would be impractical to train an entirely new LLM and providing real-time updates on breaking news would be near impossible.\n",
            "\n",
            "=== Retrieval Augmented Generation\n",
            "\n",
            "A news agency could train a model on static data and supplement the pre-existing knowledge base with real-time data retrieved from up-to-date sources.\n",
            "\n",
            "This approach is known as **Retrieval Augmented Generation**, or **RAG** 0.7298355102539062\n",
            "On the other hand, a lower temperature, even `0`, is required when a response grounded in facts is essential.\n",
            "\n",
            "[TIP]\n",
            ".Consider the correct temperature\n",
            "====\n",
            "In June 2023, link:https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/[A US judged sanctioned two US lawyers for submitting an LLM-generated legal brief^] that contained six fictitious case citations.\n",
            "====\n",
            "\n",
            "A quick fix _may_ be to reduce the temperature. But more likely, the LLM is hallucinating because it hasn't got the information required.\n",
            "\n",
            "=== Missing Information\n",
            "\n",
            "The training process for LLMs is intricate and time-intensive, often requiring vast datasets compiled over extended periods. As such, these models might lack the most recent information or might miss out on specific niche topics not well-represented in their training data.\n",
            "\n",
            "For instance, if an LLM's last update were in September 2022, it would be unaware of world events or advancements in various fields that occurred post that date, leading to potential gaps in its knowledge or responses that seem out of touch with current realities.\n",
            "\n",
            "If the user asks a question on information that is hard to find or outside of the public domain, it will be virtually impossible for an LLM to respond accurately.\n",
            "\n",
            "Luckily, this is where factual information from data sources such as knowledge graphs can help.\n",
            "\n",
            "=== Model Training and Complexity 0.7104819416999817\n",
            "image::images/llm-news-agency.svg[A news agency chatbot, showing the user asking a question, the chatbot grounding the question with a news API, and the chatbot responding with the latest news.]\n",
            "\n",
            "== LLMs and Knowledge Graphs\n",
            "\n",
            "In the coming lessons, you will explore these topics in detail and discover how LLMs can use Knowledge Graphs to improve their accuracy and relevance.\n",
            "\n",
            "== Check Your Understanding\n",
            "\n",
            "include::questions/1-temperature.adoc[leveloffset=+1]\n",
            "include::questions/2-external-data.adoc[leveloffset=+1]\n",
            "\n",
            "\n",
            "[.summary]\n",
            "== Lesson Summary\n",
            "\n",
            "In this lesson, you explored the intricacies of Large Language Models (LLMs), understanding their tendencies to hallucinate and the various strategies to improve their accuracy, such as temperature settings, prompt engineering, in-context learning, fine-tuning, and grounding with external data sources like APIs.\n",
            "\n",
            "In the next lesson, you will learn about the techniques for _grounding_ an LLM. 0.7050879597663879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOPPsYmVOey2",
        "outputId": "c920a3f8-1e79-471e-9842-80a94ec52aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREATING MY OWN EMBEDDINGS**"
      ],
      "metadata": {
        "id": "opG0kCGnz8dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "id": "wajouhD_s6z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "phrase = \"You can extract topics from phrases using TextBlob\"\n",
        "\n",
        "topics = TextBlob(phrase).noun_phrases\n",
        "\n",
        "print(topics)"
      ],
      "metadata": {
        "id": "9gzCMW53s_IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(llm, text):\n",
        "    response = llm.embed_content(\n",
        "            content=text,\n",
        "            model=\"models/text-embedding-004\"\n",
        "        )\n",
        "    return response[\"embedding\"]"
      ],
      "metadata": {
        "id": "JHvghFvDVaQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_course_data(llm, chunk):\n",
        "    data = {}\n",
        "\n",
        "    path = chunk.metadata['source'].split(os.path.sep)\n",
        "\n",
        "    data['course'] = path[-6]\n",
        "    data['module'] = path[-4]\n",
        "    data['lesson'] = path[-2]\n",
        "    data['url'] = f\"https://graphacademy.neo4j.com/courses/{data['course']}/{data['module']}/{data['lesson']}\"\n",
        "    data['text'] = chunk.page_content\n",
        "    data['embedding'] = get_embedding(llm, data['text'])\n",
        "    data['topics'] = TextBlob(data['text']).noun_phrases\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "xfSkdO3UWGoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver = GraphDatabase.driver(\n",
        "    neo_url,\n",
        "    auth=(\n",
        "        neo_user,\n",
        "        neo_pass\n",
        "    )\n",
        ")\n",
        "driver.verify_connectivity()"
      ],
      "metadata": {
        "id": "frO4iC6XWnoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chunk(tx, data):\n",
        "    tx.run(\"\"\"\n",
        "        MERGE (c:Course {name: $course})\n",
        "        MERGE (c)-[:HAS_MODULE]->(m:Module{name: $module})\n",
        "        MERGE (m)-[:HAS_LESSON]->(l:Lesson{name: $lesson, url: $url})\n",
        "        MERGE (l)-[:CONTAINS]->(p:Paragraph{text: $text})\n",
        "        WITH p\n",
        "        CALL db.create.setNodeVectorProperty(p, \"embedding\", $embedding)\n",
        "        FOREACH ( topic in $topics |\n",
        "        MERGE (p)-[:MENTIONS]->(t:Topic{name: topic})\n",
        "        )\n",
        "        \"\"\",\n",
        "        data\n",
        "        )"
      ],
      "metadata": {
        "id": "sRd65TliW8xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = genai\n",
        "for chunk in chunks:\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "\n",
        "        session.execute_write(\n",
        "            create_chunk,\n",
        "            get_course_data(llm, chunk)\n",
        "        )"
      ],
      "metadata": {
        "id": "UahK3laWW_k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with driver.session(database=\"neo4j\") as session:\n",
        "    session.run(\"\"\"\n",
        "        CREATE VECTOR INDEX paragraphs IF NOT EXISTS\n",
        "        FOR (p:Paragraph) ON (p.embedding)\n",
        "        OPTIONS { indexConfig: {\n",
        "         `vector.dimensions`: 768,\n",
        "          `vector.similarity_function`: 'cosine'\n",
        "        }}\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "DjgDwG2Et50p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver.close()"
      ],
      "metadata": {
        "id": "v5kvxrQMYWS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "my_vector = Neo4jVector.from_existing_index(\n",
        "    GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key = gem_key),\n",
        "    url=neo_url,\n",
        "    username=neo_user,\n",
        "    password=neo_pass,\n",
        "    node_label='Paragraph',\n",
        "    index_name = 'paragraphs',\n",
        "    embedding_node_property=\"embedding\",\n",
        "    text_node_property=\"text\",\n",
        ")"
      ],
      "metadata": {
        "id": "6lKQFGcYiBoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=gem_key)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=my_vector.as_retriever(),\n",
        "    verbose=True,\n",
        "    return_source_documents=True)"
      ],
      "metadata": {
        "id": "NX4I496unkxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa_chain.invoke({'query':\"What does hallucination mean?\"})\n",
        "print(\"Query: \", response['query'])\n",
        "print('\\n\\n')\n",
        "print(\"Response:\", response['result'])\n",
        "print('\\n\\n')\n",
        "print(\"=\"*200)\n",
        "print(\"Sources: \")\n",
        "last_page = \"\"\n",
        "for doc in response['source_documents']:\n",
        "    print(doc.page_content)\n",
        "    print(\"=\"*200)\n",
        "    print('\\n\\n')\n",
        "    last_page = doc.page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5B44m3Nn2-M",
        "outputId": "c6f2c0e6-02c1-482f-979f-269bfa388567",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Query:  What does hallucination mean?\n",
            "\n",
            "\n",
            "\n",
            "Response: In the context of Large Language Models (LLMs), **hallucination** refers to the phenomenon where the model generates outputs that are factually incorrect, lack logical coherence, or are based on biases in the training data. \n",
            "\n",
            "It's like the model is \"making things up\" or creating information that doesn't exist in reality. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "========================================================================================================================================================================================================\n",
            "Sources: \n",
            "= Avoiding Hallucination\n",
            ":order: 2\n",
            ":type: lesson\n",
            "\n",
            "As you learned in the previous lesson, LLMs can \"make things up\".\n",
            "\n",
            "LLMs are designed to generate human-like text based on the patterns they've identified in vast amounts of data. \n",
            "\n",
            "Due to their reliance on patterns and the sheer volume of training information, LLMs sometimes **hallucinate** or produce outputs that manifest as generating untrue facts, asserting details with unwarranted confidence, or crafting plausible yet nonsensical explanations.\n",
            "\n",
            "These manifestations arise from a mix of _overfitting_, biases in the training data, and the model's attempt to generalize from vast amounts of information.\n",
            "\n",
            "== Common Hallucination Problems\n",
            "\n",
            "Let's take a closer look at some reasons why this may occur.\n",
            "\n",
            "=== Temperature\n",
            "\n",
            "LLMs have a _temperature_, corresponding to the amount of randomness the underlying model should use when generating the text.\n",
            "\n",
            "The higher the temperature value, the more random the generated result will become, and the more likely the response will contain false statements.\n",
            "\n",
            "A higher temperature may be appropriate when configuring an LLM to respond with more diverse and creative outputs, but it comes at the expense of consistency and precision.\n",
            "\n",
            "For example, a higher temperature may be suitable for constructing a work of fiction or a novel joke.\n",
            "\n",
            "On the other hand, a lower temperature, even `0`, is required when a response grounded in facts is essential.\n",
            "========================================================================================================================================================================================================\n",
            "\n",
            "\n",
            "\n",
            "A knowledge graph is a mechanism for providing additional data to an LLM. \n",
            "Data within the knowledge graph can guide the LLM to provide more relevant, accurate, and reliable responses. \n",
            "\n",
            "While the LLM uses its language skills to interpret and respond to the contextual data, it will not disregard the original training data.\n",
            "\n",
            "You can think of the original training data as the base knowledge and linguistic capabilities, while the contextual information guides in specific situations.\n",
            "\n",
            "The combination of both approaches enables the LLM to generate more meaningful responses.\n",
            "\n",
            "== Check Your Understanding\n",
            "\n",
            "include::questions/1-hallucination.adoc[leveloffset=+1]\n",
            "include::questions/2-fixing-hallucination.adoc[leveloffset=+1]\n",
            "\n",
            "\n",
            "[.summary]\n",
            "== Lesson Summary\n",
            "\n",
            "In this lesson, you learned about LLMs, their benefits, and challenges.\n",
            "\n",
            "In the next lesson, you will learn about hallucination and the strategies for avoiding it.\n",
            "========================================================================================================================================================================================================\n",
            "\n",
            "\n",
            "\n",
            "LLMs are fine-tuned to be as helpful as possible, even if that means occasionally generating misleading or baseless content, a phenomenon known as **hallucination**.\n",
            "\n",
            "For example, when asked to _\"Describe the moon.\"_ an LLM may respond with _\"The moon is made of cheese.\"_. While this is a common saying, it is not true.\n",
            "\n",
            "image::images/confused-llm.svg[A diagram of a confused LLM with a question mark thinking about the moon and cheese.]\n",
            "\n",
            "While LLMs can represent the essence of words and phrases, they don't possess a genuine understanding or ethical judgment of the content.\n",
            "\n",
            "Large Language Models (LLMs) are often considered \"black boxes\" due to the difficulty deciphering their decision-making processes.\n",
            "The LLM would also be unable to provide the sources for its output or explain its reasoning.\n",
            "\n",
            "image::images/llm-blackbox.svg[An LLM as a black box, responding to the question 'How did you determine that answer?' with 'I don't know.']\n",
            "\n",
            "These factors can lead to outputs that might be biased, devoid of context, or lack logical coherence.\n",
            "\n",
            "=== Fixing Hallucinations\n",
            "\n",
            "Providing additional *contextual* data helps to _ground_ the LLM's responses and make them more accurate.\n",
            "\n",
            "A knowledge graph is a mechanism for providing additional data to an LLM. \n",
            "Data within the knowledge graph can guide the LLM to provide more relevant, accurate, and reliable responses.\n",
            "========================================================================================================================================================================================================\n",
            "\n",
            "\n",
            "\n",
            "= Grounding LLMs\n",
            ":order: 3\n",
            ":type: lesson\n",
            "\n",
            "In the previous lesson, you learned about the potential methods that you could use to avoid _Hallucination_, including _Grounding_.\n",
            "\n",
            "In this lesson, you will explore grounding and an approach known as _Retrieval Augmented Generation_ (RAG).\n",
            "\n",
            "[.video]\n",
            "video::Le_j18w2oGI[youtube,width=560,height=315]\n",
            "\n",
            "[.transcript]\n",
            "== Grounding LLMs with RAG\n",
            "\n",
            "Grounding is the process of providing context to an LLM to improve the accuracy of its responses and reduce the likelihood of hallucinations. \n",
            "For developers and data scientists, grounding usually offers the highest impact for the lowest effort.\n",
            "\n",
            "Training a Large Language Model has a high computational cost. \n",
            "According to Wikipedia, OpenAI's GPT-3 model was link:https://en.wikipedia.org/wiki/GPT-3[trained on 175 billion parameters^], and the resulting trained model takes 800GB to store.\n",
            "\n",
            "Retraining a model on new data would be expensive and time-consuming. \n",
            "A model may take weeks or months to train.\n",
            "\n",
            "For example, if you were building a chatbot for a news agency it would be impractical to train an entirely new LLM and providing real-time updates on breaking news would be near impossible.\n",
            "\n",
            "=== Retrieval Augmented Generation\n",
            "\n",
            "A news agency could train a model on static data and supplement the pre-existing knowledge base with real-time data retrieved from up-to-date sources.\n",
            "\n",
            "This approach is known as **Retrieval Augmented Generation**, or **RAG**\n",
            "========================================================================================================================================================================================================\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(last_page)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5rv8feh-0sJ",
        "outputId": "88f9465a-ed1a-4d4c-b1ac-d548338bce97",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= Grounding LLMs\n",
            ":order: 3\n",
            ":type: lesson\n",
            "\n",
            "In the previous lesson, you learned about the potential methods that you could use to avoid _Hallucination_, including _Grounding_.\n",
            "\n",
            "In this lesson, you will explore grounding and an approach known as _Retrieval Augmented Generation_ (RAG).\n",
            "\n",
            "[.video]\n",
            "video::Le_j18w2oGI[youtube,width=560,height=315]\n",
            "\n",
            "[.transcript]\n",
            "== Grounding LLMs with RAG\n",
            "\n",
            "Grounding is the process of providing context to an LLM to improve the accuracy of its responses and reduce the likelihood of hallucinations. \n",
            "For developers and data scientists, grounding usually offers the highest impact for the lowest effort.\n",
            "\n",
            "Training a Large Language Model has a high computational cost. \n",
            "According to Wikipedia, OpenAI's GPT-3 model was link:https://en.wikipedia.org/wiki/GPT-3[trained on 175 billion parameters^], and the resulting trained model takes 800GB to store.\n",
            "\n",
            "Retraining a model on new data would be expensive and time-consuming. \n",
            "A model may take weeks or months to train.\n",
            "\n",
            "For example, if you were building a chatbot for a news agency it would be impractical to train an entirely new LLM and providing real-time updates on breaking news would be near impossible.\n",
            "\n",
            "=== Retrieval Augmented Generation\n",
            "\n",
            "A news agency could train a model on static data and supplement the pre-existing knowledge base with real-time data retrieved from up-to-date sources.\n",
            "\n",
            "This approach is known as **Retrieval Augmented Generation**, or **RAG**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with driver.session(database=\"neo4j\") as session:\n",
        "    response = session.run(\"MATCH (:Paragraph {text: $last_page})-[:MENTIONS]->(t:Topic) RETURN count(t)\", {\"last_page\": last_page})\n",
        "\n",
        "    for record in response:\n",
        "        print(record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnWg6LXj-4xX",
        "outputId": "8a064703-fc69-47ec-f6b0-a1685c8ad713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Record count(t)=30>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-f729d14f7634>:1: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
            "  with driver.session(database=\"neo4j\") as session:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph = Neo4jGraph(\n",
        "    url=neo_url,\n",
        "    username=neo_user,\n",
        "    password=neo_pass,\n",
        ")\n"
      ],
      "metadata": {
        "id": "LuU-4xsFpuuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "resp = genai.embed_content(\n",
        "    model=\"models/text-embedding-004\",\n",
        "    content=\"What is hallucination?\"\n",
        ")\n",
        "\n",
        "embedding = resp[\"embedding\"]\n",
        "print(type(embedding))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "aQQ5dRjtqHqf",
        "outputId": "0233c148-df92-4cac-a2be-031a0a1be571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resp = graph.query(\"\"\"\n",
        "CALL db.index.vector.queryNodes('paragraphs', 6, $embedding)\n",
        "YIELD node, score\n",
        "RETURN node.text, score\n",
        "\"\"\", {\"embedding\": embedding})\n",
        "\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwCgDSUhp8Hd",
        "outputId": "6c6baacb-aed3-4f71-cd63-f2abb7462626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'node.text': '= Avoiding Hallucination\\n:order: 2\\n:type: lesson\\n\\nAs you learned in the previous lesson, LLMs can \"make things up\".\\n\\nLLMs are designed to generate human-like text based on the patterns they\\'ve identified in vast amounts of data. \\n\\nDue to their reliance on patterns and the sheer volume of training information, LLMs sometimes **hallucinate** or produce outputs that manifest as generating untrue facts, asserting details with unwarranted confidence, or crafting plausible yet nonsensical explanations.\\n\\nThese manifestations arise from a mix of _overfitting_, biases in the training data, and the model\\'s attempt to generalize from vast amounts of information.\\n\\n== Common Hallucination Problems\\n\\nLet\\'s take a closer look at some reasons why this may occur.\\n\\n=== Temperature\\n\\nLLMs have a _temperature_, corresponding to the amount of randomness the underlying model should use when generating the text.\\n\\nThe higher the temperature value, the more random the generated result will become, and the more likely the response will contain false statements.\\n\\nA higher temperature may be appropriate when configuring an LLM to respond with more diverse and creative outputs, but it comes at the expense of consistency and precision.\\n\\nFor example, a higher temperature may be suitable for constructing a work of fiction or a novel joke.\\n\\nOn the other hand, a lower temperature, even `0`, is required when a response grounded in facts is essential.', 'score': 0.8001236915588379}, {'node.text': 'A knowledge graph is a mechanism for providing additional data to an LLM. \\nData within the knowledge graph can guide the LLM to provide more relevant, accurate, and reliable responses. \\n\\nWhile the LLM uses its language skills to interpret and respond to the contextual data, it will not disregard the original training data.\\n\\nYou can think of the original training data as the base knowledge and linguistic capabilities, while the contextual information guides in specific situations.\\n\\nThe combination of both approaches enables the LLM to generate more meaningful responses.\\n\\n== Check Your Understanding\\n\\ninclude::questions/1-hallucination.adoc[leveloffset=+1]\\ninclude::questions/2-fixing-hallucination.adoc[leveloffset=+1]\\n\\n\\n[.summary]\\n== Lesson Summary\\n\\nIn this lesson, you learned about LLMs, their benefits, and challenges.\\n\\nIn the next lesson, you will learn about hallucination and the strategies for avoiding it.', 'score': 0.7812221050262451}, {'node.text': 'LLMs are fine-tuned to be as helpful as possible, even if that means occasionally generating misleading or baseless content, a phenomenon known as **hallucination**.\\n\\nFor example, when asked to _\"Describe the moon.\"_ an LLM may respond with _\"The moon is made of cheese.\"_. While this is a common saying, it is not true.\\n\\nimage::images/confused-llm.svg[A diagram of a confused LLM with a question mark thinking about the moon and cheese.]\\n\\nWhile LLMs can represent the essence of words and phrases, they don\\'t possess a genuine understanding or ethical judgment of the content.\\n\\nLarge Language Models (LLMs) are often considered \"black boxes\" due to the difficulty deciphering their decision-making processes.\\nThe LLM would also be unable to provide the sources for its output or explain its reasoning.\\n\\nimage::images/llm-blackbox.svg[An LLM as a black box, responding to the question \\'How did you determine that answer?\\' with \\'I don\\'t know.\\']\\n\\nThese factors can lead to outputs that might be biased, devoid of context, or lack logical coherence.\\n\\n=== Fixing Hallucinations\\n\\nProviding additional *contextual* data helps to _ground_ the LLM\\'s responses and make them more accurate.\\n\\nA knowledge graph is a mechanism for providing additional data to an LLM. \\nData within the knowledge graph can guide the LLM to provide more relevant, accurate, and reliable responses.', 'score': 0.7697644233703613}, {'node.text': \"= Grounding LLMs\\n:order: 3\\n:type: lesson\\n\\nIn the previous lesson, you learned about the potential methods that you could use to avoid _Hallucination_, including _Grounding_.\\n\\nIn this lesson, you will explore grounding and an approach known as _Retrieval Augmented Generation_ (RAG).\\n\\n[.video]\\nvideo::Le_j18w2oGI[youtube,width=560,height=315]\\n\\n[.transcript]\\n== Grounding LLMs with RAG\\n\\nGrounding is the process of providing context to an LLM to improve the accuracy of its responses and reduce the likelihood of hallucinations. \\nFor developers and data scientists, grounding usually offers the highest impact for the lowest effort.\\n\\nTraining a Large Language Model has a high computational cost. \\nAccording to Wikipedia, OpenAI's GPT-3 model was link:https://en.wikipedia.org/wiki/GPT-3[trained on 175 billion parameters^], and the resulting trained model takes 800GB to store.\\n\\nRetraining a model on new data would be expensive and time-consuming. \\nA model may take weeks or months to train.\\n\\nFor example, if you were building a chatbot for a news agency it would be impractical to train an entirely new LLM and providing real-time updates on breaking news would be near impossible.\\n\\n=== Retrieval Augmented Generation\\n\\nA news agency could train a model on static data and supplement the pre-existing knowledge base with real-time data retrieved from up-to-date sources.\\n\\nThis approach is known as **Retrieval Augmented Generation**, or **RAG**\", 'score': 0.7303831577301025}, {'node.text': \"On the other hand, a lower temperature, even `0`, is required when a response grounded in facts is essential.\\n\\n[TIP]\\n.Consider the correct temperature\\n====\\nIn June 2023, link:https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/[A US judged sanctioned two US lawyers for submitting an LLM-generated legal brief^] that contained six fictitious case citations.\\n====\\n\\nA quick fix _may_ be to reduce the temperature. But more likely, the LLM is hallucinating because it hasn't got the information required.\\n\\n=== Missing Information\\n\\nThe training process for LLMs is intricate and time-intensive, often requiring vast datasets compiled over extended periods. As such, these models might lack the most recent information or might miss out on specific niche topics not well-represented in their training data.\\n\\nFor instance, if an LLM's last update were in September 2022, it would be unaware of world events or advancements in various fields that occurred post that date, leading to potential gaps in its knowledge or responses that seem out of touch with current realities.\\n\\nIf the user asks a question on information that is hard to find or outside of the public domain, it will be virtually impossible for an LLM to respond accurately.\\n\\nLuckily, this is where factual information from data sources such as knowledge graphs can help.\\n\\n=== Model Training and Complexity\", 'score': 0.7048637866973877}, {'node.text': 'image::images/llm-news-agency.svg[A news agency chatbot, showing the user asking a question, the chatbot grounding the question with a news API, and the chatbot responding with the latest news.]\\n\\n== LLMs and Knowledge Graphs\\n\\nIn the coming lessons, you will explore these topics in detail and discover how LLMs can use Knowledge Graphs to improve their accuracy and relevance.\\n\\n== Check Your Understanding\\n\\ninclude::questions/1-temperature.adoc[leveloffset=+1]\\ninclude::questions/2-external-data.adoc[leveloffset=+1]\\n\\n\\n[.summary]\\n== Lesson Summary\\n\\nIn this lesson, you explored the intricacies of Large Language Models (LLMs), understanding their tendencies to hallucinate and the various strategies to improve their accuracy, such as temperature settings, prompt engineering, in-context learning, fine-tuning, and grounding with external data sources like APIs.\\n\\nIn the next lesson, you will learn about the techniques for _grounding_ an LLM.', 'score': 0.6979150772094727}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import GraphCypherQAChain\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "graph = Neo4jGraph(\n",
        "    url=neo_url,\n",
        "    username=neo_user,\n",
        "    password=neo_pass,\n",
        ")\n",
        "\n",
        "cypher_chain = GraphCypherQAChain.from_llm(\n",
        "    llm,\n",
        "    graph=graph,\n",
        "    verbose=True,\n",
        "    allow_dangerous_requests=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "WOcRR-5C278A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph.schema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufchdG2D3bFi",
        "outputId": "baef1e8a-9306-4b7e-c7fa-e807bc36bc24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node properties:\n",
            "Course {name: STRING}\n",
            "Module {name: STRING}\n",
            "Lesson {name: STRING, url: STRING}\n",
            "Paragraph {text: STRING, embedding: LIST}\n",
            "Topic {name: STRING}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "(:Course)-[:HAS_MODULE]->(:Module)\n",
            "(:Module)-[:HAS_LESSON]->(:Lesson)\n",
            "(:Lesson)-[:CONTAINS]->(:Paragraph)\n",
            "(:Paragraph)-[:MENTIONS]->(:Topic)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How many Lessons contain paragraphs that mention the topic vector index\"\n",
        "result = cypher_chain.invoke({\"query\": query})\n",
        "print(result['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_JgSvAf3qAC",
        "outputId": "2c99e7f1-ae0c-4da4-9eab-be6373194018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mcypher\n",
            "MATCH (l:Lesson)-[:CONTAINS]->(p:Paragraph)-[:MENTIONS]->(t:Topic {name: \"vector index\"})\n",
            "RETURN count(DISTINCT l)\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'count(DISTINCT l)': 3}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "There are 3 lessons that contain paragraphs mentioning the topic vector index. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is average number topics each paragraph mentions\"\n",
        "result = cypher_chain.invoke({\"query\": query})\n",
        "print(result['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBcF2WwV3zjR",
        "outputId": "f1750b55-ffc6-45a8-b606-4fdb75f15014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mcypher\n",
            "MATCH (p:Paragraph)-[:MENTIONS]->(t:Topic)\n",
            "WITH p, count(t) AS topic_count\n",
            "RETURN avg(topic_count)\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'avg(topic_count)': 27.89887640449438}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The average number of topics each paragraph mentions is 27.89887640449438. \n",
            "\n"
          ]
        }
      ]
    }
  ]
}